{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fahim-03/NLP_Labs/blob/main/lab1_preprocessing_tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UYc27BU4uCSG"
      },
      "outputs": [],
      "source": [
        "__author__ = \"Pranava Madhyastha\"\n",
        "__version__ = \"INM434/IN3045 City, University of London, Spring 2026\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenisation: a brief overview\n",
        "\n",
        "This is our first notebook for the module and we are going to focus on tokenisation. Please make sure to login to your respective google account to get started.\n",
        "\n",
        "A tokenizer is used to split an input string into separate tokens or pieces, where each token represents a meaningful element in the input string. This is one of the most important steps in NLP. This step is also called text-preprocessing.\n",
        "\n",
        "Tokenisation helps create a vocabulary of unique tokens overwhich we will run a variety of NLP algorithms. It mainly helps standardise the input data and help us squeeze in information from the long tail. It, in some cases, helps us to break the input text down into smaller, sometimes linguistically meaningful, units.\n",
        "\n",
        "In this notebook, we will first build a simple tokeniser which only splits on white space. We will then look at a toy morphological analyser. We will then build a simplified version of subword based tokeniser.\n",
        "\n",
        "---\n",
        "\n",
        "How does this notebook work:\n",
        "\n",
        "  * There will be some cells with \"TODO\": you will have to fill the code for the placeholder \"YOUR CODE HERE\".\n",
        "  * You will also notice \"ADVANCED TODO\", this is for students who are indeed capable of writing\n",
        "\n",
        "  * Each cell should take a few seconds to run, so if it is taking longer, there may be bug."
      ],
      "metadata": {
        "id": "1l01hmqxu_aR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple tokeniser\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "We will make use of the built in regular expression library in python to construct this tokeniser. Please refer to Chapter 2 (SLP: Jurafsky and Martin) references on regular expressions.\n",
        "\n",
        "The tokeniser below will take split input text into separate words based on word boundaries. This is defined using the regular expression pattern \"\\b\\w+\\b\". The re.findall() function is used to extract all the tokens from the text that match the pattern, and the list of tokens is returned.\n"
      ],
      "metadata": {
        "id": "RJQMAos2vaI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def tokenise(text):\n",
        "    # Define a regular expression pattern to split the text into tokens\n",
        "    pattern = r'\\b\\w+\\b'\n",
        "\n",
        "    # Use the re.findall() function to extract all the tokens from the text\n",
        "    tokens = re.findall(pattern, text)\n",
        "\n",
        "    # Return the list of tokens\n",
        "    return tokens\n",
        "\n",
        "# Sample input for the tokeniser\n",
        "text = \"This is IN 3045/INM 434 Natural Language Procssing. This is some sample text for tokeniser\"\n",
        "tokens = tokenise(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "WEsmbqQWvddu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3cb8a11-6e1f-4da0-e042-adac5aa0dddc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'IN', '3045', 'INM', '434', 'Natural', 'Language', 'Procssing', 'This', 'is', 'some', 'sample', 'text', 'for', 'tokeniser']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: What does '\\b\\w+\\b' mean?\n",
        "\n",
        "It splits a series of alphanumerical characters by whitespace"
      ],
      "metadata": {
        "id": "3TsA2hnFvhPO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Building a Sentiment Classifier <a name=\"classifier\"></a>\n",
        "\n",
        "Let's build a sentiment classifier that predicts whether a movie review is **positive** or **negative** -- this is one of the classic NLP tasks.\n",
        "\n",
        "## Our Plan is:\n",
        "\n",
        "1. **Tokenise**: Input the text by tokenising the text\n",
        "2. **Vectorise**: Convert the elements of the text into a bag-of-words representation\n",
        "3. **Train**: Learn which words indicate positive/negative sentiment\n",
        "4. **Predict**: Classify new reviews"
      ],
      "metadata": {
        "id": "h4rVnmIWvl40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "from collections import Counter, defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Our training data: movie reviews with sentiment labels\n",
        "training_data = [\n",
        "    # Positive reviews\n",
        "    (\"This movie is amazing and wonderful\", \"positive\"),\n",
        "    (\"I loved every minute of this film\", \"positive\"),\n",
        "    (\"Brilliant acting and great story\", \"positive\"),\n",
        "    (\"One of the best movies I have seen\", \"positive\"),\n",
        "    (\"Absolutely fantastic and entertaining\", \"positive\"),\n",
        "    (\"A beautiful and moving experience\", \"positive\"),\n",
        "    (\"Highly recommend this masterpiece\", \"positive\"),\n",
        "    (\"Excellent film with superb performances\", \"positive\"),\n",
        "    (\"I really enjoyed this movie\", \"positive\"),\n",
        "    (\"What a great and fun movie\", \"positive\"),\n",
        "    (\"The movie was not bad\", \"positive\"),\n",
        "    (\"I did not hate it\", \"positive\"), # TEACH: NOT_hate is positive\n",
        "\n",
        "    # Negative reviews\n",
        "    (\"This movie is terrible and boring\", \"negative\"),\n",
        "    (\"I hated every minute of this film\", \"negative\"),\n",
        "    (\"Awful acting and stupid story\", \"negative\"),\n",
        "    (\"One of the worst movies ever made\", \"negative\"),\n",
        "    (\"Absolutely dreadful waste of time\", \"negative\"),\n",
        "    (\"A painful and tedious experience\", \"negative\"),\n",
        "    (\"Do not waste your money on this\", \"negative\"),\n",
        "    (\"Horrible film with bad performances\", \"negative\"),\n",
        "    (\"I really disliked this movie\", \"negative\"),\n",
        "    (\"This movie is not good\", \"negative\"), # TEACH: NOT_good is negative\n",
        "    (\"I don't like this film\", \"negative\"), # TEACH: NOT_like is negative\n",
        "]\n",
        "\n",
        "pos_count = 0\n",
        "neg_count = 0\n",
        "\n",
        "for data in training_data:\n",
        "  if(data[1] == \"positive\"):\n",
        "    pos_count += 1\n",
        "  else:\n",
        "    neg_count += 1\n",
        "\n",
        "print(f\"Total training examples: {len(training_data)}\")\n",
        "print(f\"Total positive reviews: {pos_count}\")\n",
        "print(f\"Total negative reviews: {neg_count}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "g4KvIFL2vjrr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7324d4f3-16b7-44c8-dcb8-925721c79a60"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training examples: 23\n",
            "Total positive reviews: 12\n",
            "Total negative reviews: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Please write code to list the number of training examples in total, with the number of positive and negative examples."
      ],
      "metadata": {
        "id": "ommNHxaMwNsu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Tokenizer\n",
        "\n",
        "# First, we need to split text into words. Let's start with a simple approach:"
      ],
      "metadata": {
        "id": "R9XNg_ldwHaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_tokenize(text):\n",
        "    # TODO - write code to convert to lowercase -> just ended up using pythons built-in lower() function\n",
        "    # Write code to Split on word boundaries, keep only alphanumeric\n",
        "    pattern = r'\\b\\w+\\b'\n",
        "    tokens = re.findall(pattern, text.lower())\n",
        "\n",
        "    return tokens\n",
        "\n",
        "print(simple_tokenize(\"This is IN 3045/INM 434 Natural Language Procssing. This is some sample text for tokeniser\"))"
      ],
      "metadata": {
        "id": "QbXs63CKwEm-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6af4bfe2-0b81-4a5b-e9c1-6e0619d5668a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['this', 'is', 'in', '3045', 'inm', '434', 'natural', 'language', 'procssing', 'this', 'is', 'some', 'sample', 'text', 'for', 'tokeniser']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Vocabulary\n",
        "\n",
        "We need to know all unique words in our training data -- this is the VOCABULARY of our model"
      ],
      "metadata": {
        "id": "ryww5nJhwyOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocabulary(data, tokenize_fn):\n",
        "\n",
        "    # we will use COUNTER class from python -- https://docs.python.org/3/library/collections.html\n",
        "    word_counts = Counter()\n",
        "\n",
        "    for text, label in data:\n",
        "        tokens = tokenize_fn(text)\n",
        "        word_counts.update(tokens)\n",
        "\n",
        "    # TODO: What is counter doing? -> it creates a map/dict of all tokens in a corpus/text, in the format token: Frequency\n",
        "\n",
        "    # Create word-to-index mapping\n",
        "    vocab = {word: idx for idx, word in enumerate(sorted(word_counts.keys()))}\n",
        "\n",
        "    return vocab, word_counts\n",
        "\n",
        "# Build vocabulary\n",
        "vocab, word_counts = build_vocabulary(training_data, simple_tokenize)\n",
        "print(f\"Vocabulary size: {len(vocab)}\")\n",
        "print(vocab)\n",
        "print(\"Words from Most to Least frequent:\")\n",
        "for token, freq in word_counts.most_common():\n",
        "  print(f\"- {token}: {freq}\")\n"
      ],
      "metadata": {
        "id": "0gY9bC7Mwutt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75dc1a9a-8e01-4ba5-a234-0640826aad11"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 61\n",
            "{'a': 0, 'absolutely': 1, 'acting': 2, 'amazing': 3, 'and': 4, 'awful': 5, 'bad': 6, 'beautiful': 7, 'best': 8, 'boring': 9, 'brilliant': 10, 'disliked': 11, 'do': 12, 'dreadful': 13, 'enjoyed': 14, 'entertaining': 15, 'ever': 16, 'every': 17, 'excellent': 18, 'experience': 19, 'fantastic': 20, 'film': 21, 'fun': 22, 'great': 23, 'hated': 24, 'have': 25, 'highly': 26, 'horrible': 27, 'i': 28, 'is': 29, 'loved': 30, 'made': 31, 'masterpiece': 32, 'minute': 33, 'money': 34, 'movie': 35, 'movies': 36, 'moving': 37, 'not': 38, 'of': 39, 'on': 40, 'one': 41, 'painful': 42, 'performances': 43, 'really': 44, 'recommend': 45, 'seen': 46, 'story': 47, 'stupid': 48, 'superb': 49, 'tedious': 50, 'terrible': 51, 'the': 52, 'this': 53, 'time': 54, 'waste': 55, 'what': 56, 'with': 57, 'wonderful': 58, 'worst': 59, 'your': 60}\n",
            "Words from Most to Least frequent:\n",
            "- this: 8\n",
            "- and: 8\n",
            "- movie: 5\n",
            "- i: 5\n",
            "- of: 5\n",
            "- film: 4\n",
            "- a: 3\n",
            "- is: 2\n",
            "- every: 2\n",
            "- minute: 2\n",
            "- acting: 2\n",
            "- great: 2\n",
            "- story: 2\n",
            "- one: 2\n",
            "- the: 2\n",
            "- movies: 2\n",
            "- absolutely: 2\n",
            "- experience: 2\n",
            "- with: 2\n",
            "- performances: 2\n",
            "- really: 2\n",
            "- waste: 2\n",
            "- amazing: 1\n",
            "- wonderful: 1\n",
            "- loved: 1\n",
            "- brilliant: 1\n",
            "- best: 1\n",
            "- have: 1\n",
            "- seen: 1\n",
            "- fantastic: 1\n",
            "- entertaining: 1\n",
            "- beautiful: 1\n",
            "- moving: 1\n",
            "- highly: 1\n",
            "- recommend: 1\n",
            "- masterpiece: 1\n",
            "- excellent: 1\n",
            "- superb: 1\n",
            "- enjoyed: 1\n",
            "- what: 1\n",
            "- fun: 1\n",
            "- terrible: 1\n",
            "- boring: 1\n",
            "- hated: 1\n",
            "- awful: 1\n",
            "- stupid: 1\n",
            "- worst: 1\n",
            "- ever: 1\n",
            "- made: 1\n",
            "- dreadful: 1\n",
            "- time: 1\n",
            "- painful: 1\n",
            "- tedious: 1\n",
            "- do: 1\n",
            "- not: 1\n",
            "- your: 1\n",
            "- money: 1\n",
            "- on: 1\n",
            "- horrible: 1\n",
            "- bad: 1\n",
            "- disliked: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Write code to print the size of the vocabulary and also list the most frequent to least frequent words."
      ],
      "metadata": {
        "id": "na1A7j49xjmY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag-of-Words Representation\n",
        "\n",
        "Convert each document to a vector (in our case an array) of word counts"
      ],
      "metadata": {
        "id": "kykeFFGSx5_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_bow(text, vocab, tokenize_fn):\n",
        "    # Initialize vector of zeros\n",
        "    bow = np.zeros(len(vocab))\n",
        "\n",
        "    # Count words\n",
        "    tokens = tokenize_fn(text)\n",
        "    for token in tokens:\n",
        "        if token in vocab:\n",
        "            bow[vocab[token]] += 1\n",
        "\n",
        "    return bow\n",
        "\n",
        "\n",
        "example  = \"This movie is \"\n",
        "bow_vector = text_to_bow(example, vocab, simple_tokenize)\n",
        "print (bow_vector)\n",
        "print (\"All tokens with non-zero entries:\")\n",
        "for token, index in vocab.items():\n",
        "    if bow_vector[index] > 0:\n",
        "        print(f\"- {token}\")"
      ],
      "metadata": {
        "id": "lzBrPl_ExpZN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4be35c8-fb60-4c5c-f189-9ab54b9c9dce"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "All tokens with non-zero entries:\n",
            "- is\n",
            "- movie\n",
            "- this\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Print out the bag of word vector. Also, print out all the \"tokens\" with non-zero entries."
      ],
      "metadata": {
        "id": "JGY-Fw2lybpS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now Let us train a Simple Classifier\n",
        "\n",
        "We will now use a very simple classifier -- the Naive Bayes classifier which is a simple but effective classifier for text.\n",
        "\n",
        "The idea: estimate P(word | positive) and P(word | negative), then use Bayes' rule.\n",
        "\n",
        "Essentially we are seeing If a review is positive, will it have the word 'good'?. Then, we ask, if I indeed see the word 'good', what is the probability the review is positive?. It allows us to update our belief about the review's sentiment as we read each word -- this is a fairly naive way of approaching language -- here we don't care about the rich structure!"
      ],
      "metadata": {
        "id": "Dhj1XdiCzQgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NaiveBayesClassifier:\n",
        "\n",
        "\n",
        "    def __init__(self, tokenize_fn):\n",
        "        self.tokenize_fn = tokenize_fn\n",
        "        self.vocab = None\n",
        "        self.word_probs = {}  # P(word | class)\n",
        "        self.class_probs = {}  # P(class)\n",
        "\n",
        "    def train(self, data):\n",
        "\n",
        "        # Build vocabulary\n",
        "        self.vocab, word_counts = build_vocabulary(data, self.tokenize_fn)\n",
        "\n",
        "        # Count words per class -- TODO: Why do we need this?\n",
        "        # (ANSWER) This is needed to calculate P(word|class) as we want to know the probabilty of a word being in a class given all words in a class\n",
        "        class_word_counts = defaultdict(Counter)\n",
        "        class_doc_counts = Counter()\n",
        "\n",
        "        # Remember each item in data is a string, label pair\n",
        "        for text, label in data:\n",
        "            tokens = self.tokenize_fn(text)\n",
        "            class_word_counts[label].update(tokens)\n",
        "            class_doc_counts[label] += 1\n",
        "\n",
        "        # Calculate probabilities with smoothing -- this is done to prevent division by zero!\n",
        "        total_docs = len(data)\n",
        "        vocab_size = len(self.vocab)\n",
        "\n",
        "        for label in class_doc_counts:\n",
        "            # P(class)\n",
        "            self.class_probs[label] = class_doc_counts[label] / total_docs\n",
        "\n",
        "            # P(word | class) with Laplace smoothing\n",
        "            total_words = sum(class_word_counts[label].values())\n",
        "            self.word_probs[label] = {}\n",
        "\n",
        "            for word in self.vocab:\n",
        "                count = class_word_counts[label][word]\n",
        "                # Laplace smoothing -- add 1 to avoid zero probabilities\n",
        "                self.word_probs[label][word] = (count + 1) / (total_words + vocab_size)\n",
        "\n",
        "        print(f\"Trained on {total_docs} documents\")\n",
        "        print(f\"Vocabulary size is: {vocab_size}\")\n",
        "        print(f\"Total number of classes: {list(class_doc_counts.keys())}\")\n",
        "\n",
        "    def predict(self, text, return_probs=False):\n",
        "\n",
        "        tokens = self.tokenize_fn(text)\n",
        "\n",
        "        # Calculate log probability for each class\n",
        "        log_probs = {}\n",
        "\n",
        "        for label in self.class_probs:\n",
        "            # Start with log P(class)\n",
        "            log_prob = np.log(self.class_probs[label])\n",
        "\n",
        "            # Add log P(word | class) for each word\n",
        "            for token in tokens:\n",
        "                if token in self.word_probs[label]:\n",
        "                    log_prob += np.log(self.word_probs[label][token])\n",
        "                # Ignore unknown words (or could use a small probability)\n",
        "\n",
        "            log_probs[label] = log_prob\n",
        "\n",
        "        # Predict class with highest probability\n",
        "        predicted = max(log_probs, key=log_probs.get)\n",
        "\n",
        "        if return_probs:\n",
        "            # Convert to actual probabilities\n",
        "            max_log = max(log_probs.values())\n",
        "            probs = {k: np.exp(v - max_log) for k, v in log_probs.items()}\n",
        "            total = sum(probs.values())\n",
        "            probs = {k: v/total for k, v in probs.items()}\n",
        "            return predicted, probs\n",
        "\n",
        "        return predicted\n",
        "\n",
        "    def get_important_words(self, n=10):\n",
        "\n",
        "        important = {}\n",
        "\n",
        "        for label in self.class_probs:\n",
        "            other_label = [l for l in self.class_probs if l != label][0]\n",
        "\n",
        "            ratios = []\n",
        "            for word in self.vocab:\n",
        "                ratio = self.word_probs[label][word] / self.word_probs[other_label][word]\n",
        "                ratios.append((word, ratio))\n",
        "\n",
        "            ratios.sort(key=lambda x: x[1], reverse=True)\n",
        "            important[label] = ratios[:n]\n",
        "\n",
        "        return important\n",
        "\n",
        "# Let us now train our classifier\n",
        "classifier = NaiveBayesClassifier(simple_tokenize)\n",
        "classifier.train(training_data)"
      ],
      "metadata": {
        "id": "2xulYqYn1Nd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13ce7a9a-10dc-4163-9652-81b57780ff08"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained on 19 documents\n",
            "Vocabulary size is: 61\n",
            "Total number of classes: ['positive', 'negative']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## So what has it learned?\n",
        "\n",
        "Let us see which words are most indicative of positive vs negative sentiment?"
      ],
      "metadata": {
        "id": "dImRnZe13jAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "important_words = classifier.get_important_words(10)\n",
        "#print(important_words)\n",
        "print(\"Most important words and their ratios:\")\n",
        "for label, word_ratios in important_words.items():\n",
        "  print(f\"\\n--- {label.upper()} ---\")\n",
        "  for word, ratio in word_ratios:\n",
        "    print(f\"- Word: '{word}', Ratio: {ratio:.4f}\")"
      ],
      "metadata": {
        "id": "qlq2e2cs33v8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "991157f5-eb47-42d4-d32d-bb2fe78ea9e0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most important words and their ratios:\n",
            "\n",
            "--- POSITIVE ---\n",
            "- Word: 'great', Ratio: 2.9224\n",
            "- Word: 'amazing', Ratio: 1.9483\n",
            "- Word: 'beautiful', Ratio: 1.9483\n",
            "- Word: 'best', Ratio: 1.9483\n",
            "- Word: 'brilliant', Ratio: 1.9483\n",
            "- Word: 'enjoyed', Ratio: 1.9483\n",
            "- Word: 'entertaining', Ratio: 1.9483\n",
            "- Word: 'excellent', Ratio: 1.9483\n",
            "- Word: 'fantastic', Ratio: 1.9483\n",
            "- Word: 'fun', Ratio: 1.9483\n",
            "\n",
            "--- NEGATIVE ---\n",
            "- Word: 'waste', Ratio: 3.0796\n",
            "- Word: 'awful', Ratio: 2.0531\n",
            "- Word: 'bad', Ratio: 2.0531\n",
            "- Word: 'boring', Ratio: 2.0531\n",
            "- Word: 'disliked', Ratio: 2.0531\n",
            "- Word: 'do', Ratio: 2.0531\n",
            "- Word: 'dreadful', Ratio: 2.0531\n",
            "- Word: 'ever', Ratio: 2.0531\n",
            "- Word: 'hated', Ratio: 2.0531\n",
            "- Word: 'horrible', Ratio: 2.0531\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Print out the most important words and their corresponding ratios"
      ],
      "metadata": {
        "id": "UMbH-Ki6355k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Does it work? <a name=\"test\"></a>\n",
        "\n",
        "Let's test our classifier on some examples:"
      ],
      "metadata": {
        "id": "pUKlvcef4Xxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_examples = [\n",
        "    \"This movie is amazing\",\n",
        "    \"I loved this film\",\n",
        "    \"Terrible and boring\",\n",
        "    \"The worst movie ever\",\n",
        "    \"A great and entertaining film\",\n",
        "    \"Awful waste of time\",\n",
        "]\n",
        "\n",
        "print(\"Testing our classifier:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for text in test_examples:\n",
        "    pred, probs = classifier.predict(text, return_probs=True)\n",
        "    confidence = probs[pred] * 100\n",
        "    emoji = \"correct\" if pred == \"positive\" else \"incorrect\"\n",
        "    print(f\"{emoji} '{text}'\")\n",
        "    print(f\"   --> {pred} ({confidence:.1f}% confident)\\n\")"
      ],
      "metadata": {
        "id": "ppDmvToP4VjG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "501e678c-a048-411c-c47c-6d3ab6e118fa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing our classifier:\n",
            "============================================================\n",
            "correct 'This movie is amazing'\n",
            "   --> positive (72.7% confident)\n",
            "\n",
            "correct 'I loved this film'\n",
            "   --> positive (72.7% confident)\n",
            "\n",
            "incorrect 'Terrible and boring'\n",
            "   --> negative (72.2% confident)\n",
            "\n",
            "incorrect 'The worst movie ever'\n",
            "   --> negative (75.0% confident)\n",
            "\n",
            "correct 'A great and entertaining film'\n",
            "   --> positive (92.9% confident)\n",
            "\n",
            "incorrect 'Awful waste of time'\n",
            "   --> negative (94.1% confident)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How did it do?\n",
        "\n",
        "Our simple bag-of-words classifier correctly identifies sentiment for straightforward cases.\n",
        "\n",
        "Even this simple approach works because:\n",
        "- Positive reviews tend to use words like \"amazing\", \"loved\", \"great\"\n",
        "- Negative reviews tend to use words like \"terrible\", \"awful\", \"worst\"\n",
        "\n",
        "Now let's look at some tricky cases!"
      ],
      "metadata": {
        "id": "kMxT9h515a7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tricky_examples = [\n",
        "    # Negation ones\n",
        "    (\"This movie is not good\", \"negative\"),\n",
        "    (\"I don't like this film\", \"negative\"),\n",
        "    (\"Not bad at all\", \"positive\"),\n",
        "    (\"I didn't hate it\", \"positive\"),\n",
        "\n",
        "    # Say there is some sarcasm or contrasting bits\n",
        "    (\"Oh great, another terrible movie\", \"negative\"),\n",
        "    (\"The acting was good but the story was awful\", \"mixed\"),\n",
        "\n",
        "    # Say we have words not seen in the data ?\n",
        "    (\"This film is phenomenal\", \"positive\"),\n",
        "    (\"Utterly abysmal\", \"negative\"),\n",
        "    (\"A cinematic triumph\", \"positive\"),\n",
        "\n",
        "    # Intensity bits\n",
        "    (\"This movie is good\", \"positive\"),\n",
        "    (\"This movie is REALLY good\", \"positive\"),\n",
        "    (\"This movie is soooo good!!!\", \"positive\"),\n",
        "\n",
        "    # Word variations\n",
        "    (\"I loved it\", \"positive\"),\n",
        "    (\"I'm loving it\", \"positive\"),\n",
        "    (\"Lovable characters\", \"positive\"),\n",
        "]\n",
        "\n",
        "print(\"Testing on TRICKY examples:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "correct = 0\n",
        "wrong = 0\n",
        "\n",
        "for text, expected in tricky_examples:\n",
        "    pred, probs = classifier.predict(text, return_probs=True)\n",
        "    confidence = probs[pred] * 100\n",
        "\n",
        "    if expected == \"mixed\":\n",
        "        status = \"dont know\"  # Can't really be right or wrong\n",
        "    elif pred == expected:\n",
        "        status = \"correct\"\n",
        "        correct += 1\n",
        "    else:\n",
        "        status = \"incorrect\"\n",
        "        wrong += 1\n",
        "\n",
        "    print(f\"{status} '{text}'\")\n",
        "    print(f\"   Expected: {expected}, Got: {pred} ({confidence:.1f}%)\\n\")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(f\"Results: {correct} correct, {wrong} wrong (excluding 'mixed')\")"
      ],
      "metadata": {
        "id": "OBlUzGp65ibo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "083b6cc1-ecb5-484f-ec44-5d83a69ae295"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing on TRICKY examples:\n",
            "======================================================================\n",
            "correct 'This movie is not good'\n",
            "   Expected: negative, Got: negative (60.0%)\n",
            "\n",
            "incorrect 'I don't like this film'\n",
            "   Expected: negative, Got: positive (57.8%)\n",
            "\n",
            "incorrect 'Not bad at all'\n",
            "   Expected: positive, Got: negative (79.1%)\n",
            "\n",
            "correct 'I didn't hate it'\n",
            "   Expected: positive, Got: positive (59.1%)\n",
            "\n",
            "incorrect 'Oh great, another terrible movie'\n",
            "   Expected: negative, Got: positive (67.3%)\n",
            "\n",
            "dont know 'The acting was good but the story was awful'\n",
            "   Expected: mixed, Got: negative (67.2%)\n",
            "\n",
            "correct 'This film is phenomenal'\n",
            "   Expected: positive, Got: positive (50.7%)\n",
            "\n",
            "incorrect 'Utterly abysmal'\n",
            "   Expected: negative, Got: positive (52.6%)\n",
            "\n",
            "correct 'A cinematic triumph'\n",
            "   Expected: positive, Got: positive (61.9%)\n",
            "\n",
            "correct 'This movie is good'\n",
            "   Expected: positive, Got: positive (57.8%)\n",
            "\n",
            "correct 'This movie is REALLY good'\n",
            "   Expected: positive, Got: positive (57.2%)\n",
            "\n",
            "correct 'This movie is soooo good!!!'\n",
            "   Expected: positive, Got: positive (57.8%)\n",
            "\n",
            "correct 'I loved it'\n",
            "   Expected: positive, Got: positive (73.8%)\n",
            "\n",
            "correct 'I'm loving it'\n",
            "   Expected: positive, Got: positive (59.1%)\n",
            "\n",
            "correct 'Lovable characters'\n",
            "   Expected: positive, Got: positive (52.6%)\n",
            "\n",
            "======================================================================\n",
            "Results: 10 correct, 4 wrong (excluding 'mixed')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced TODO:\n",
        "Can we come up with a negation aware tokenizer? That is -- say you tokenize such that \"the movie is not good\" is tokenized as [\"the\", \"movie\", \"is\", \"not\",  \"NOT_good\"]."
      ],
      "metadata": {
        "id": "uNBxGhT36D17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "# 1. Create a new classifier with tokenize_with_negation\n",
        "# 2. Train it on training_data\n",
        "# 3. Test on the negation examples from tricky_examples\n",
        "def tokenize_with_negation(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub (r'\\b(not|don\\'t)\\s+(\\w+)', r'\\1 NOT_\\2', text)\n",
        "  pattern = r'\\b\\w+\\b'\n",
        "  tokens = re.findall(pattern, text)\n",
        "\n",
        "  return tokens\n",
        "\n",
        "classifier_negation = NaiveBayesClassifier(tokenize_with_negation)\n",
        "classifier_negation.train(training_data)\n",
        "\n",
        "# Test on negation examples\n",
        "negation_tests = [\n",
        "    (\"This movie is not good\", \"negative\"),\n",
        "    (\"I don't like this film\", \"negative\"),\n",
        "    (\"Not bad at all\", \"positive\"), # This will fail until we teach the model that not negative means positive\n",
        "]\n",
        "\n",
        "print(\"Testing negation-aware tokenizer:\")\n",
        "for text, expected in negation_tests:\n",
        "    tokens = tokenize_with_negation(text)\n",
        "    pred = classifier_negation.predict(text)\n",
        "    status = \"correct\" if pred == expected else \"incorrect\"\n",
        "    print(f\"{status} '{text}'\")\n",
        "    print(f\"   Tokens: {tokens}\")\n",
        "    print(f\"   Expected: {expected}, Got: {pred}\\n\")"
      ],
      "metadata": {
        "id": "5jDHOlUG7fZM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95d032dc-90d8-4dba-8191-fe0cc4e4bb9e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained on 23 documents\n",
            "Vocabulary size is: 71\n",
            "Total number of classes: ['positive', 'negative']\n",
            "Testing negation-aware tokenizer:\n",
            "correct 'This movie is not good'\n",
            "   Tokens: ['this', 'movie', 'is', 'not', 'NOT_good']\n",
            "   Expected: negative, Got: negative\n",
            "\n",
            "correct 'I don't like this film'\n",
            "   Tokens: ['i', 'don', 't', 'NOT_like', 'this', 'film']\n",
            "   Expected: negative, Got: negative\n",
            "\n",
            "correct 'Not bad at all'\n",
            "   Tokens: ['not', 'NOT_bad', 'at', 'all']\n",
            "   Expected: positive, Got: positive\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Better Tokenization with BPE <a name=\"part5\"></a>\n",
        "\n",
        "**Byte-Pair Encoding (BPE)** learns a vocabulary from data that balances:\n",
        "- Small vocabulary size\n",
        "- Meaningful units (not just characters)\n",
        "- No OOV problem (can always fall back to characters)\n",
        "\n",
        "This is what GPT, BERT, and most modern LLMs use!\n",
        "\n",
        "\n",
        "Sennrich, Rico, Barry Haddow, and Alexandra Birch. \"Neural Machine Translation of Rare Words with Subword Units.\" Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Vol. 1. 2016. http://www.aclweb.org/anthology/P16-1162\n",
        "\n",
        "Main source: https://github.com/rsennrich/subword-nmt\n",
        "\n",
        "Below is a simple version of the algorithm.\n",
        "\n",
        "Two stages: token learner and token segmenter.\n",
        "\n",
        "Let us first look at token learner, this involves:\n",
        "\n",
        "*  computing the frequencies of all words in a corpus (we do it synthetically here)\n",
        "*  start with characters as the basic vocab (characters seen in the corpus)\n",
        "* to obtain vocabulary of n-merge operations:\n",
        "    - Obtain most frequent pairs of characters in the corpus\n",
        "    - add the pair to the list of merges\n",
        "    - add merged characters to the vocab\n",
        "* iterate n times\n",
        "\n",
        "The code below performs this operation."
      ],
      "metadata": {
        "id": "m77JSqgF8A3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "def compute_pair_frequencies(vocab):\n",
        "\n",
        "    pairs = collections.defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols) - 1):\n",
        "            pairs[symbols[i], symbols[i + 1]] += freq\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def merge_vocab(pair, vocab):\n",
        "\n",
        "    new_vocab = {}\n",
        "    bigram = re.escape(' '.join(pair))\n",
        "    pattern = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "\n",
        "    for word in vocab:\n",
        "        new_word = pattern.sub(''.join(pair), word)\n",
        "        new_vocab[new_word] = vocab[word]\n",
        "\n",
        "    return new_vocab\n",
        "\n",
        "\n",
        "def train_bpe(corpus_words, num_merges):\n",
        "\n",
        "    # Initialize: split each word into characters + end marker\n",
        "    vocab = {}\n",
        "    for word, freq in corpus_words.items():\n",
        "        symbols = ' '.join(list(word)) + ' </w>'\n",
        "        vocab[symbols] = freq\n",
        "\n",
        "    bpe_codes = {}\n",
        "\n",
        "    print(\"BPE Training\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Initial vocabulary: {vocab}\\n\")\n",
        "\n",
        "    for i in range(num_merges):\n",
        "        pairs = compute_pair_frequencies(vocab)\n",
        "\n",
        "        if not pairs:\n",
        "            print(\"No more pairs to merge!\")\n",
        "            break\n",
        "\n",
        "        # Find most frequent pair\n",
        "        best_pair = max(pairs, key=pairs.get)\n",
        "        best_freq = pairs[best_pair]\n",
        "\n",
        "        # Merge these!\n",
        "        vocab = merge_vocab(best_pair, vocab)\n",
        "        bpe_codes[best_pair] = i\n",
        "\n",
        "        print(f\"Merge {i+1}: '{best_pair[0]}' + '{best_pair[1]}' -> '{best_pair[0]+best_pair[1]}' (freq: {best_freq})\")\n",
        "\n",
        "    print(f\"\\nFinal vocabulary: {vocab}\")\n",
        "\n",
        "    return bpe_codes, vocab"
      ],
      "metadata": {
        "id": "WJ7UFSi98Hy2"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to train it -- that is, it learns from data!"
      ],
      "metadata": {
        "id": "woqmYoop8la9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train BPE on a small corpus\n",
        "corpus_words = {\n",
        "    'low': 5,\n",
        "    'lower': 2,\n",
        "    'newest': 6,\n",
        "    'widest': 3,\n",
        "    'new': 4,\n",
        "}\n",
        "\n",
        "bpe_codes, final_vocab = train_bpe(corpus_words, num_merges=10)"
      ],
      "metadata": {
        "id": "lwzF9SX98iqq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1b77030-dc70-4e7d-a352-00fe91b796a8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BPE Training\n",
            "============================================================\n",
            "Initial vocabulary: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3, 'n e w </w>': 4}\n",
            "\n",
            "Merge 1: 'n' + 'e' -> 'ne' (freq: 10)\n",
            "Merge 2: 'ne' + 'w' -> 'new' (freq: 10)\n",
            "Merge 3: 'e' + 's' -> 'es' (freq: 9)\n",
            "Merge 4: 'es' + 't' -> 'est' (freq: 9)\n",
            "Merge 5: 'est' + '</w>' -> 'est</w>' (freq: 9)\n",
            "Merge 6: 'l' + 'o' -> 'lo' (freq: 7)\n",
            "Merge 7: 'lo' + 'w' -> 'low' (freq: 7)\n",
            "Merge 8: 'new' + 'est</w>' -> 'newest</w>' (freq: 6)\n",
            "Merge 9: 'low' + '</w>' -> 'low</w>' (freq: 5)\n",
            "Merge 10: 'new' + '</w>' -> 'new</w>' (freq: 4)\n",
            "\n",
            "Final vocabulary: {'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3, 'new</w>': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def bpe_encode(word, bpe_codes):\n",
        "\n",
        "    # Start with characters + end marker\n",
        "    word = tuple(word) + ('</w>',)\n",
        "\n",
        "    while len(word) > 1:\n",
        "        # Find all adjacent pairs\n",
        "        pairs = [(word[i], word[i+1]) for i in range(len(word)-1)]\n",
        "\n",
        "        # Find pair with lowest merge rank (earliest learned)\n",
        "        min_pair = None\n",
        "        min_rank = float('inf')\n",
        "\n",
        "        for pair in pairs:\n",
        "            if pair in bpe_codes and bpe_codes[pair] < min_rank:\n",
        "                min_pair = pair\n",
        "                min_rank = bpe_codes[pair]\n",
        "\n",
        "        # If no pair can be merged, we're done\n",
        "        if min_pair is None:\n",
        "            break\n",
        "\n",
        "        # Merge all occurrences of min_pair\n",
        "        first, second = min_pair\n",
        "        new_word = []\n",
        "        i = 0\n",
        "\n",
        "        while i < len(word):\n",
        "            if i < len(word) - 1 and word[i] == first and word[i+1] == second:\n",
        "                new_word.append(first + second)\n",
        "                i += 2\n",
        "            else:\n",
        "                new_word.append(word[i])\n",
        "                i += 1\n",
        "\n",
        "        word = tuple(new_word)\n",
        "\n",
        "    # Clean up end marker for display\n",
        "    result = []\n",
        "    for token in word:\n",
        "        if token == '</w>':\n",
        "            continue\n",
        "        elif token.endswith('</w>'):\n",
        "            result.append(token[:-4])\n",
        "        else:\n",
        "            result.append(token)\n",
        "\n",
        "    return tuple(result)\n",
        "\n",
        "\n",
        "# Test BPE encoding\n",
        "test_words = ['low', 'lower', 'lowest', 'new', 'newer', 'newest', 'widest', 'unknown']\n",
        "\n",
        "print(\"\\nBPE Encoding Results:\")\n",
        "print(\"=\" * 40)\n",
        "for word in test_words:\n",
        "    encoded = bpe_encode(word, bpe_codes)\n",
        "    print(f\"{word:<12} -> {encoded}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WD1_fAwV8wFZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fedaa33d-fb01-464c-8560-2bc31d606dda"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BPE Encoding Results:\n",
            "========================================\n",
            "low          -> ('low',)\n",
            "lower        -> ('low', 'e', 'r')\n",
            "lowest       -> ('low', 'est')\n",
            "new          -> ('new',)\n",
            "newer        -> ('new', 'e', 'r')\n",
            "newest       -> ('newest',)\n",
            "widest       -> ('w', 'i', 'd', 'est')\n",
            "unknown      -> ('u', 'n', 'k', 'n', 'o', 'w', 'n')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, observe something interesting (even in this small set of examples). The BPE tokenizer has learned to split the words into:\n",
        "- `est` (superlative suffix)\n",
        "- `er` (comparative suffix, eventually)\n",
        "- `low`, `new` (common roots)\n",
        "\n",
        "This happened automatically from frequency, not from any linguistic rules!\n",
        "\n",
        "But, this may not be always true, as you see in the outputs!  "
      ],
      "metadata": {
        "id": "wVD8DAPn88zE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ADVANCED TODO: Train BPE on the full corpus above."
      ],
      "metadata": {
        "id": "S0kp6t1x9O-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Prepare the data\n",
        "# We need to convert our list of sentences (training_data) into a dictionary of word counts\n",
        "from collections import Counter\n",
        "\n",
        "full_corpus_counts = Counter()\n",
        "\n",
        "for text, label in training_data:\n",
        "    # Use the simple tokenizer we built earlier to get the words\n",
        "    words = simple_tokenize(text)\n",
        "    full_corpus_counts.update(words)\n",
        "\n",
        "print(f\"Total unique words in corpus: {len(full_corpus_counts)}\")\n",
        "print(f\"Top 5 words: {full_corpus_counts.most_common(5)}\")\n",
        "\n",
        "# 2. Train BPE\n",
        "# Let's run 50 merges (you can increase this number to learn longer words)\n",
        "print(\"\\n--- Starting BPE Training ---\")\n",
        "my_bpe_codes, my_bpe_vocab = train_bpe(full_corpus_counts, num_merges=50)\n",
        "\n",
        "# 3. Test it\n",
        "print(\"\\n--- Testing BPE Encoding ---\")\n",
        "test_words = [\"movie\", \"movies\", \"moving\", \"movement\", \"wonderful\", \"wonderfully\"]\n",
        "\n",
        "for word in test_words:\n",
        "    encoded = bpe_encode(word, my_bpe_codes)\n",
        "    print(f\"{word:<12} -> {encoded}\")"
      ],
      "metadata": {
        "id": "z9KoiaT_8z3x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25a1fa51-a815-4a7c-db1f-c98f4aed8c69"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique words in corpus: 69\n",
            "Top 5 words: [('this', 10), ('and', 8), ('movie', 7), ('i', 7), ('of', 5)]\n",
            "\n",
            "--- Starting BPE Training ---\n",
            "BPE Training\n",
            "============================================================\n",
            "Initial vocabulary: {'t h i s </w>': 10, 'm o v i e </w>': 7, 'i s </w>': 3, 'a m a z i n g </w>': 1, 'a n d </w>': 8, 'w o n d e r f u l </w>': 1, 'i </w>': 7, 'l o v e d </w>': 1, 'e v e r y </w>': 2, 'm i n u t e </w>': 2, 'o f </w>': 5, 'f i l m </w>': 5, 'b r i l l i a n t </w>': 1, 'a c t i n g </w>': 2, 'g r e a t </w>': 2, 's t o r y </w>': 2, 'o n e </w>': 2, 't h e </w>': 3, 'b e s t </w>': 1, 'm o v i e s </w>': 2, 'h a v e </w>': 1, 's e e n </w>': 1, 'a b s o l u t e l y </w>': 2, 'f a n t a s t i c </w>': 1, 'e n t e r t a i n i n g </w>': 1, 'a </w>': 3, 'b e a u t i f u l </w>': 1, 'm o v i n g </w>': 1, 'e x p e r i e n c e </w>': 2, 'h i g h l y </w>': 1, 'r e c o m m e n d </w>': 1, 'm a s t e r p i e c e </w>': 1, 'e x c e l l e n t </w>': 1, 'w i t h </w>': 2, 's u p e r b </w>': 1, 'p e r f o r m a n c e s </w>': 2, 'r e a l l y </w>': 2, 'e n j o y e d </w>': 1, 'w h a t </w>': 1, 'f u n </w>': 1, 'w a s </w>': 1, 'n o t </w>': 4, 'b a d </w>': 2, 'd i d </w>': 1, 'h a t e </w>': 1, 'i t </w>': 1, 't e r r i b l e </w>': 1, 'b o r i n g </w>': 1, 'h a t e d </w>': 1, 'a w f u l </w>': 1, 's t u p i d </w>': 1, 'w o r s t </w>': 1, 'e v e r </w>': 1, 'm a d e </w>': 1, 'd r e a d f u l </w>': 1, 'w a s t e </w>': 2, 't i m e </w>': 1, 'p a i n f u l </w>': 1, 't e d i o u s </w>': 1, 'd o </w>': 1, 'y o u r </w>': 1, 'm o n e y </w>': 1, 'o n </w>': 1, 'h o r r i b l e </w>': 1, 'd i s l i k e d </w>': 1, 'g o o d </w>': 1, 'd o n </w>': 1, 't </w>': 1, 'l i k e </w>': 1}\n",
            "\n",
            "Merge 1: 'e' + '</w>' -> 'e</w>' (freq: 26)\n",
            "Merge 2: 's' + '</w>' -> 's</w>' (freq: 19)\n",
            "Merge 3: 'd' + '</w>' -> 'd</w>' (freq: 18)\n",
            "Merge 4: 't' + 'h' -> 'th' (freq: 15)\n",
            "Merge 5: 'i' + 's</w>' -> 'is</w>' (freq: 13)\n",
            "Merge 6: 't' + '</w>' -> 't</w>' (freq: 13)\n",
            "Merge 7: 'a' + 'n' -> 'an' (freq: 12)\n",
            "Merge 8: 'e' + 'r' -> 'er' (freq: 12)\n",
            "Merge 9: 'm' + 'o' -> 'mo' (freq: 11)\n",
            "Merge 10: 'th' + 'is</w>' -> 'this</w>' (freq: 10)\n",
            "Merge 11: 'mo' + 'v' -> 'mov' (freq: 10)\n",
            "Merge 12: 'mov' + 'i' -> 'movi' (freq: 10)\n",
            "Merge 13: 'y' + '</w>' -> 'y</w>' (freq: 10)\n",
            "Merge 14: 'i' + 'n' -> 'in' (freq: 9)\n",
            "Merge 15: 'an' + 'd</w>' -> 'and</w>' (freq: 8)\n",
            "Merge 16: 'movi' + 'e</w>' -> 'movie</w>' (freq: 7)\n",
            "Merge 17: 'i' + '</w>' -> 'i</w>' (freq: 7)\n",
            "Merge 18: 's' + 't' -> 'st' (freq: 7)\n",
            "Merge 19: 'o' + 'r' -> 'or' (freq: 7)\n",
            "Merge 20: 'e' + 'n' -> 'en' (freq: 7)\n",
            "Merge 21: 'g' + '</w>' -> 'g</w>' (freq: 6)\n",
            "Merge 22: 'f' + 'u' -> 'fu' (freq: 6)\n",
            "Merge 23: 'i' + 'l' -> 'il' (freq: 6)\n",
            "Merge 24: 'r' + 'e' -> 're' (freq: 6)\n",
            "Merge 25: 'in' + 'g</w>' -> 'ing</w>' (freq: 5)\n",
            "Merge 26: 'o' + 'n' -> 'on' (freq: 5)\n",
            "Merge 27: 'fu' + 'l' -> 'ful' (freq: 5)\n",
            "Merge 28: 'ful' + '</w>' -> 'ful</w>' (freq: 5)\n",
            "Merge 29: 'u' + 't' -> 'ut' (freq: 5)\n",
            "Merge 30: 'o' + 'f' -> 'of' (freq: 5)\n",
            "Merge 31: 'of' + '</w>' -> 'of</w>' (freq: 5)\n",
            "Merge 32: 'f' + 'il' -> 'fil' (freq: 5)\n",
            "Merge 33: 'fil' + 'm' -> 'film' (freq: 5)\n",
            "Merge 34: 'film' + '</w>' -> 'film</w>' (freq: 5)\n",
            "Merge 35: 're' + 'a' -> 'rea' (freq: 5)\n",
            "Merge 36: 'l' + 'y</w>' -> 'ly</w>' (freq: 5)\n",
            "Merge 37: 'p' + 'er' -> 'per' (freq: 5)\n",
            "Merge 38: 'e' + 'd</w>' -> 'ed</w>' (freq: 4)\n",
            "Merge 39: 'e' + 's</w>' -> 'es</w>' (freq: 4)\n",
            "Merge 40: 'h' + 'a' -> 'ha' (freq: 4)\n",
            "Merge 41: 'a' + 'st' -> 'ast' (freq: 4)\n",
            "Merge 42: 'n' + 'o' -> 'no' (freq: 4)\n",
            "Merge 43: 'no' + 't</w>' -> 'not</w>' (freq: 4)\n",
            "Merge 44: 'e' + 'v' -> 'ev' (freq: 3)\n",
            "Merge 45: 'ev' + 'er' -> 'ever' (freq: 3)\n",
            "Merge 46: 'l' + 'i' -> 'li' (freq: 3)\n",
            "Merge 47: 'th' + 'e</w>' -> 'the</w>' (freq: 3)\n",
            "Merge 48: 'a' + '</w>' -> 'a</w>' (freq: 3)\n",
            "Merge 49: 'e' + 'x' -> 'ex' (freq: 3)\n",
            "Merge 50: 'c' + 'e</w>' -> 'ce</w>' (freq: 3)\n",
            "\n",
            "Final vocabulary: {'this</w>': 10, 'movie</w>': 7, 'is</w>': 3, 'a m a z ing</w>': 1, 'and</w>': 8, 'w on d er ful</w>': 1, 'i</w>': 7, 'l o v ed</w>': 1, 'ever y</w>': 2, 'm in ut e</w>': 2, 'of</w>': 5, 'film</w>': 5, 'b r il li an t</w>': 1, 'a c t ing</w>': 2, 'g rea t</w>': 2, 'st or y</w>': 2, 'on e</w>': 2, 'the</w>': 3, 'b e s t</w>': 1, 'movi es</w>': 2, 'ha v e</w>': 1, 's e en </w>': 1, 'a b s o l ut e ly</w>': 2, 'f an t ast i c </w>': 1, 'en t er t a in ing</w>': 1, 'a</w>': 3, 'b e a ut i ful</w>': 1, 'movi n g</w>': 1, 'ex per i en ce</w>': 2, 'h i g h ly</w>': 1, 're c o m m en d</w>': 1, 'm ast er p i e ce</w>': 1, 'ex c e l l en t</w>': 1, 'w i th </w>': 2, 's u per b </w>': 1, 'per f or m an c es</w>': 2, 'rea l ly</w>': 2, 'en j o y ed</w>': 1, 'w ha t</w>': 1, 'fu n </w>': 1, 'w a s</w>': 1, 'not</w>': 4, 'b a d</w>': 2, 'd i d</w>': 1, 'ha t e</w>': 1, 'i t</w>': 1, 't er r i b l e</w>': 1, 'b or ing</w>': 1, 'ha t ed</w>': 1, 'a w ful</w>': 1, 'st u p i d</w>': 1, 'w or s t</w>': 1, 'ever </w>': 1, 'm a d e</w>': 1, 'd rea d ful</w>': 1, 'w ast e</w>': 2, 't i m e</w>': 1, 'p a in ful</w>': 1, 't e d i o u s</w>': 1, 'd o </w>': 1, 'y o u r </w>': 1, 'mo n e y</w>': 1, 'on </w>': 1, 'h or r i b l e</w>': 1, 'd i s li k ed</w>': 1, 'g o o d</w>': 1, 'd on </w>': 1, 't</w>': 1, 'li k e</w>': 1}\n",
            "\n",
            "--- Testing BPE Encoding ---\n",
            "movie        -> ('movie',)\n",
            "movies       -> ('movi', 'es')\n",
            "moving       -> ('movi', 'n', 'g')\n",
            "movement     -> ('mov', 'e', 'm', 'en', 't')\n",
            "wonderful    -> ('w', 'on', 'd', 'er', 'ful')\n",
            "wonderfully  -> ('w', 'on', 'd', 'er', 'ful', 'ly')\n"
          ]
        }
      ]
    }
  ]
}